// Auto-generated file - do not edit manually
// Run 'npm run generate-writings' to regenerate

export const writings = [
  {
    "id": "attention-is-all-you-need",
    "slug": "attention-is-all-you-need",
    "title": "Attention is all you need",
    "excerpt": "My notes on the Transformers Architecture",
    "date": "2025-08-18",
    "category": "tech",
    "readTimeMinutes": 6,
    "content": [
      "# The Transformers Architecture \n\nIf you've been anywhere near AI in the last few years, you've probably heard about Transformers. No, not the robot movie kind (though they were pretty cool). The architecture comes from a 2017 research paper titled *\"Attention Is All You Need\"*, and it spurred frenzy in the AI world.\n\nSo, why was it groundbreaking? Let's break it down.\n\n## Traditional Transduction models: RNNs and CNNs\nBefore Transformers, traditional transduction sequence models relied on Recurrent Neural Networks [(RNN)](https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/) or Convolutional Neural Networks [(CNN)](https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/). \n\n*Transduction models*: Takes one form of sequential data and transforms it to another sequence. Kind of like a translator/convertor.\n> Example: \"Hello, how are you?\" (English) -> \"Hola, ¿cómo estás?\" (Spanish)\n\n**RNNs** read one word at a time, keeping memory of what came before. Like reading a book one word at a time, trying to remember every single detail from the beginning. By the time you hit page 200, remembering exactly what was on page 1 is… hard. This is coined the [Vanishing Gradient Problem](https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning/), long-range dependencies get fuzzy.\n\n**CNNs** were better at spotting local features, like edges in an image, but they still looked at things in \"windows.\" Like examining a giant painting through a small cut-out frame: you see details, but it's hard to appreciate the whole masterpiece at once.\n\nBoth had another big drawback in that they processed information **sequentially**, making training these models a slow, painstaking process.\n\n## The Saving Grace: Transformers\n\nTransformers flipped the script. No recurrence. No convolutions. But **Attention Mechanisms**\n\nInstead of trudging word by word, Transformers look at all the words at once. They can connect \"cat\" in the first sentence to \"it\" 50 words later instantly. This parallelism not only speeds up training but also makes it way easier to capture long-range relationships.\n\nIn short:\n\n- **RNNs** were like reading word by word and struggling with memory\n- **CNNs** were like peeking at a painting through small little frames  \n- **Transformers** let you see everything *at once*\n\n## How Attention Mechanisms Work (Without Too Much Math)\n\nThe general idea here is that every word asks: *\"Which other words in this sentence matter the most to me?\"*. It then takes into account the relative importance of every other word and embeds this information into its own numeric representation. \n\nAttention is derived from three important variables: Queries, Keys and Values. \n\nThe Q/K/V concept is analogous to retrieval systems. For example, when you search for videos on YouTube, the search engine will map your **query** (text in the search bar) against a set of **keys** (video title, description, etc.) associated with videos in the database, then present you the best matched videos (**values**) [(useful discussion)](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)\n\nIn the context of Transformers:\n- **Query (Q)**: the search question that the current word is asking\n- **Key (K)**: the searchable attributes that each word can be matched against  \n- **Value (V)**: the actual information content that gets retrieved and mixed together\n\n## Example: \"Tom eats fish\" (More Math Involved Here)\nAttention can be calculated using the following formula:\n![Project image](/writings/attention_formula.png)\n\nLet's zoom into how attention updates the word \"eats\" using sample values. \n\n(Q) Query (\"eats\" asks \"who do I relate to?\"): Let Q=1.0\n\n(K) Keys: Tom=2.0, eats=0.1, fish=1.5 -> [2.0,0.1,1.5]\n\n(V) Values:\n- Tom = [1, 0] (a subject-y feature)\n- eats = [0, 0] (self, less useful here)\n- fish = [0, 1] (an object-y feature)\n\n### Calculating Similarity Scores, Qk \nWe use the key-query dot product to understand how relevant two words are to each other. A bigger dot product = more relevant.\n\nQk = [2.0, 0.1, 1.5]\n\n### Scaling\nIf the vectors are high-dimensional, dot products can get really large, which can mess up the the values in the next step. Numbers are scaled to keep them in a nice range.\n\nAfter scaling (assume scaling function=1): [2.0, 0.1, 1.5]\n\n### Softmax Function \nNow we take all those similarity scores and run them through a softmax function. The point of this softmax function is to calculate the **attention weights** amongst the words.\n\nAfter softmax: [0.57, 0.08, 0.35]\n\n### Interpretation \nConsidering the word \"eats\", we can define how much attention it should pay to the other words in the sentence. \n- “eats” pays 57% attention to “Tom,” \n- \"eats\" pays 35% attention to “fish,” \n- \"eats\" pays 8% attention to itself\n\n### Weighted Sum of Values (V)\nFinally, we apply these attention weights to V, which hold the actual information from each word. \n- \"Tom\" V = [1, 0] (subject feature)\n- \"eats\" V = [0, 0] (verb feature)\n- \"fish\" V = [0, 1] (object feature)\nWeighted sum = 0.57·[1,0] + 0.35·[0,1] ≈ [0.57, 0.35]\n\n**This new vector becomes the updated representation of “eats”. Notice how these numbers are a reprentation of the word's relationship to both \"Tom\" and \"fish\"**\n\n### Multi-Head Attention \nInstead of doing the entire process above just once, Transformers do it in parallel across multiple “heads” with different learned Q/K/V projections. Each head can capture a different type of relationship. With several heads running in parallel.\n\nOne head might focus on **subject–verb** links. Another might look at **object–verb** links.  \n\nThese heads get combined afterwards. An analogy is like having a group of friends watching the same movie concurrently. One focuses on the plot, another on the characters and maybe another on the dialogues. Put all of their knowledge together and you get a much more intricate understanding of the entire movie.\n\n## Why This Was a Big Deal\n\nThe results spoke for themselves:\n\n- On English-to-German translation, Transformers set a new state-of-the-art with a **BLEU score of 28.4**\n- The model generalised well beyond translation, it was able to perform well in tasks like English parsing too\n\nBy solving sequence problems with attention, Transformers opened the door to models like **BERT**, **GPT**, and all the large language models we use today. That's why this one paper is cited everywhere... it marked the beginning of the modern AI wave. So, next time you hear *\"attention is all you need,\"* know it's not just a catchy phrase. It's also the research paper that revolutionised the AI landscape. \n\nAnd that's why the Transformers architecture *transformed* the tech world as we know of today (pun intended)"
    ],
    "markdown": "# The Transformers Architecture \r\n\r\nIf you've been anywhere near AI in the last few years, you've probably heard about Transformers. No, not the robot movie kind (though they were pretty cool). The architecture comes from a 2017 research paper titled *\"Attention Is All You Need\"*, and it spurred frenzy in the AI world.\r\n\r\nSo, why was it groundbreaking? Let's break it down.\r\n\r\n## Traditional Transduction models: RNNs and CNNs\r\nBefore Transformers, traditional transduction sequence models relied on Recurrent Neural Networks [(RNN)](https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/) or Convolutional Neural Networks [(CNN)](https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/). \r\n\r\n*Transduction models*: Takes one form of sequential data and transforms it to another sequence. Kind of like a translator/convertor.\r\n> Example: \"Hello, how are you?\" (English) -> \"Hola, ¿cómo estás?\" (Spanish)\r\n\r\n**RNNs** read one word at a time, keeping memory of what came before. Like reading a book one word at a time, trying to remember every single detail from the beginning. By the time you hit page 200, remembering exactly what was on page 1 is… hard. This is coined the [Vanishing Gradient Problem](https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning/), long-range dependencies get fuzzy.\r\n\r\n**CNNs** were better at spotting local features, like edges in an image, but they still looked at things in \"windows.\" Like examining a giant painting through a small cut-out frame: you see details, but it's hard to appreciate the whole masterpiece at once.\r\n\r\nBoth had another big drawback in that they processed information **sequentially**, making training these models a slow, painstaking process.\r\n\r\n## The Saving Grace: Transformers\r\n\r\nTransformers flipped the script. No recurrence. No convolutions. But **Attention Mechanisms**\r\n\r\nInstead of trudging word by word, Transformers look at all the words at once. They can connect \"cat\" in the first sentence to \"it\" 50 words later instantly. This parallelism not only speeds up training but also makes it way easier to capture long-range relationships.\r\n\r\nIn short:\r\n\r\n- **RNNs** were like reading word by word and struggling with memory\r\n- **CNNs** were like peeking at a painting through small little frames  \r\n- **Transformers** let you see everything *at once*\r\n\r\n## How Attention Mechanisms Work (Without Too Much Math)\r\n\r\nThe general idea here is that every word asks: *\"Which other words in this sentence matter the most to me?\"*. It then takes into account the relative importance of every other word and embeds this information into its own numeric representation. \r\n\r\nAttention is derived from three important variables: Queries, Keys and Values. \r\n\r\nThe Q/K/V concept is analogous to retrieval systems. For example, when you search for videos on YouTube, the search engine will map your **query** (text in the search bar) against a set of **keys** (video title, description, etc.) associated with videos in the database, then present you the best matched videos (**values**) [(useful discussion)](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)\r\n\r\nIn the context of Transformers:\r\n- **Query (Q)**: the search question that the current word is asking\r\n- **Key (K)**: the searchable attributes that each word can be matched against  \r\n- **Value (V)**: the actual information content that gets retrieved and mixed together\r\n\r\n## Example: \"Tom eats fish\" (More Math Involved Here)\r\nAttention can be calculated using the following formula:\r\n![Project image](/writings/attention_formula.png)\r\n\r\nLet's zoom into how attention updates the word \"eats\" using sample values. \r\n\r\n(Q) Query (\"eats\" asks \"who do I relate to?\"): Let Q=1.0\r\n\r\n(K) Keys: Tom=2.0, eats=0.1, fish=1.5 -> [2.0,0.1,1.5]\r\n\r\n(V) Values:\r\n- Tom = [1, 0] (a subject-y feature)\r\n- eats = [0, 0] (self, less useful here)\r\n- fish = [0, 1] (an object-y feature)\r\n\r\n### Calculating Similarity Scores, Qk \r\nWe use the key-query dot product to understand how relevant two words are to each other. A bigger dot product = more relevant.\r\n\r\nQk = [2.0, 0.1, 1.5]\r\n\r\n### Scaling\r\nIf the vectors are high-dimensional, dot products can get really large, which can mess up the the values in the next step. Numbers are scaled to keep them in a nice range.\r\n\r\nAfter scaling (assume scaling function=1): [2.0, 0.1, 1.5]\r\n\r\n### Softmax Function \r\nNow we take all those similarity scores and run them through a softmax function. The point of this softmax function is to calculate the **attention weights** amongst the words.\r\n\r\nAfter softmax: [0.57, 0.08, 0.35]\r\n\r\n### Interpretation \r\nConsidering the word \"eats\", we can define how much attention it should pay to the other words in the sentence. \r\n- “eats” pays 57% attention to “Tom,” \r\n- \"eats\" pays 35% attention to “fish,” \r\n- \"eats\" pays 8% attention to itself\r\n\r\n### Weighted Sum of Values (V)\r\nFinally, we apply these attention weights to V, which hold the actual information from each word. \r\n- \"Tom\" V = [1, 0] (subject feature)\r\n- \"eats\" V = [0, 0] (verb feature)\r\n- \"fish\" V = [0, 1] (object feature)\r\nWeighted sum = 0.57·[1,0] + 0.35·[0,1] ≈ [0.57, 0.35]\r\n\r\n**This new vector becomes the updated representation of “eats”. Notice how these numbers are a reprentation of the word's relationship to both \"Tom\" and \"fish\"**\r\n\r\n### Multi-Head Attention \r\nInstead of doing the entire process above just once, Transformers do it in parallel across multiple “heads” with different learned Q/K/V projections. Each head can capture a different type of relationship. With several heads running in parallel.\r\n\r\nOne head might focus on **subject–verb** links. Another might look at **object–verb** links.  \r\n\r\nThese heads get combined afterwards. An analogy is like having a group of friends watching the same movie concurrently. One focuses on the plot, another on the characters and maybe another on the dialogues. Put all of their knowledge together and you get a much more intricate understanding of the entire movie.\r\n\r\n## Why This Was a Big Deal\r\n\r\nThe results spoke for themselves:\r\n\r\n- On English-to-German translation, Transformers set a new state-of-the-art with a **BLEU score of 28.4**\r\n- The model generalised well beyond translation, it was able to perform well in tasks like English parsing too\r\n\r\nBy solving sequence problems with attention, Transformers opened the door to models like **BERT**, **GPT**, and all the large language models we use today. That's why this one paper is cited everywhere... it marked the beginning of the modern AI wave. So, next time you hear *\"attention is all you need,\"* know it's not just a catchy phrase. It's also the research paper that revolutionised the AI landscape. \r\n\r\nAnd that's why the Transformers architecture *transformed* the tech world as we know of today (pun intended)",
    "html": "<h1>The Transformers Architecture </h1>\n<p>If you&#39;ve been anywhere near AI in the last few years, you&#39;ve probably heard about Transformers. No, not the robot movie kind (though they were pretty cool). The architecture comes from a 2017 research paper titled <em>&quot;Attention Is All You Need&quot;</em>, and it spurred frenzy in the AI world.</p>\n<p>So, why was it groundbreaking? Let&#39;s break it down.</p>\n<h2>Traditional Transduction models: RNNs and CNNs</h2>\n<p>Before Transformers, traditional transduction sequence models relied on Recurrent Neural Networks <a href=\"https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/\" rel=\"noopener noreferrer\">(RNN)</a> or Convolutional Neural Networks <a href=\"https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/\" rel=\"noopener noreferrer\">(CNN)</a>.</p>\n<p><em>Transduction models</em>: Takes one form of sequential data and transforms it to another sequence. Kind of like a translator/convertor.</p>\n<blockquote><p>Example: &quot;Hello, how are you?&quot; (English) -&gt; &quot;Hola, ¿cómo estás?&quot; (Spanish)</p></blockquote>\n<p><strong>RNNs</strong> read one word at a time, keeping memory of what came before. Like reading a book one word at a time, trying to remember every single detail from the beginning. By the time you hit page 200, remembering exactly what was on page 1 is… hard. This is coined the <a href=\"https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning/\" rel=\"noopener noreferrer\">Vanishing Gradient Problem</a>, long-range dependencies get fuzzy.</p>\n<p><strong>CNNs</strong> were better at spotting local features, like edges in an image, but they still looked at things in &quot;windows.&quot; Like examining a giant painting through a small cut-out frame: you see details, but it&#39;s hard to appreciate the whole masterpiece at once.</p>\n<p>Both had another big drawback in that they processed information <strong>sequentially</strong>, making training these models a slow, painstaking process.</p>\n<h2>The Saving Grace: Transformers</h2>\n<p>Transformers flipped the script. No recurrence. No convolutions. But <strong>Attention Mechanisms</strong></p>\n<p>Instead of trudging word by word, Transformers look at all the words at once. They can connect &quot;cat&quot; in the first sentence to &quot;it&quot; 50 words later instantly. This parallelism not only speeds up training but also makes it way easier to capture long-range relationships.</p>\n<p>In short:</p>\n<ul><li><strong>RNNs</strong> were like reading word by word and struggling with memory</li><li><strong>CNNs</strong> were like peeking at a painting through small little frames</li><li><strong>Transformers</strong> let you see everything <em>at once</em></li></ul>\n<h2>How Attention Mechanisms Work (Without Too Much Math)</h2>\n<p>The general idea here is that every word asks: <em>&quot;Which other words in this sentence matter the most to me?&quot;</em>. It then takes into account the relative importance of every other word and embeds this information into its own numeric representation.</p>\n<p>Attention is derived from three important variables: Queries, Keys and Values.</p>\n<p>The Q/K/V concept is analogous to retrieval systems. For example, when you search for videos on YouTube, the search engine will map your <strong>query</strong> (text in the search bar) against a set of <strong>keys</strong> (video title, description, etc.) associated with videos in the database, then present you the best matched videos (<strong>values</strong>) <a href=\"https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\" rel=\"noopener noreferrer\">(useful discussion)</a></p>\n<p>In the context of Transformers:</p>\n<ul><li><strong>Query (Q)</strong>: the search question that the current word is asking</li><li><strong>Key (K)</strong>: the searchable attributes that each word can be matched against</li><li><strong>Value (V)</strong>: the actual information content that gets retrieved and mixed together</li></ul>\n<h2>Example: &quot;Tom eats fish&quot; (More Math Involved Here)</h2>\n<p>Attention can be calculated using the following formula: <img src=\"/writings/attention_formula.png\" alt=\"Project image\" /></p>\n<p>Let&#39;s zoom into how attention updates the word &quot;eats&quot; using sample values.</p>\n<p>(Q) Query (&quot;eats&quot; asks &quot;who do I relate to?&quot;): Let Q=1.0</p>\n<p>(K) Keys: Tom=2.0, eats=0.1, fish=1.5 -&gt; [2.0,0.1,1.5]</p>\n<p>(V) Values:</p>\n<ul><li>Tom = [1, 0] (a subject-y feature)</li><li>eats = [0, 0] (self, less useful here)</li><li>fish = [0, 1] (an object-y feature)</li></ul>\n<h3>Calculating Similarity Scores, Qk </h3>\n<p>We use the key-query dot product to understand how relevant two words are to each other. A bigger dot product = more relevant.</p>\n<p>Qk = [2.0, 0.1, 1.5]</p>\n<h3>Scaling</h3>\n<p>If the vectors are high-dimensional, dot products can get really large, which can mess up the the values in the next step. Numbers are scaled to keep them in a nice range.</p>\n<p>After scaling (assume scaling function=1): [2.0, 0.1, 1.5]</p>\n<h3>Softmax Function </h3>\n<p>Now we take all those similarity scores and run them through a softmax function. The point of this softmax function is to calculate the <strong>attention weights</strong> amongst the words.</p>\n<p>After softmax: [0.57, 0.08, 0.35]</p>\n<h3>Interpretation </h3>\n<p>Considering the word &quot;eats&quot;, we can define how much attention it should pay to the other words in the sentence.</p>\n<ul><li>“eats” pays 57% attention to “Tom,”</li><li>&quot;eats&quot; pays 35% attention to “fish,”</li><li>&quot;eats&quot; pays 8% attention to itself</li></ul>\n<h3>Weighted Sum of Values (V)</h3>\n<p>Finally, we apply these attention weights to V, which hold the actual information from each word.</p>\n<ul><li>&quot;Tom&quot; V = [1, 0] (subject feature)</li><li>&quot;eats&quot; V = [0, 0] (verb feature)</li><li>&quot;fish&quot; V = [0, 1] (object feature)</li></ul>\n<p>Weighted sum = 0.57·[1,0] + 0.35·[0,1] ≈ [0.57, 0.35]</p>\n<p><strong>This new vector becomes the updated representation of “eats”. Notice how these numbers are a reprentation of the word&#39;s relationship to both &quot;Tom&quot; and &quot;fish&quot;</strong></p>\n<h3>Multi-Head Attention </h3>\n<p>Instead of doing the entire process above just once, Transformers do it in parallel across multiple “heads” with different learned Q/K/V projections. Each head can capture a different type of relationship. With several heads running in parallel.</p>\n<p>One head might focus on <strong>subject–verb</strong> links. Another might look at <strong>object–verb</strong> links.</p>\n<p>These heads get combined afterwards. An analogy is like having a group of friends watching the same movie concurrently. One focuses on the plot, another on the characters and maybe another on the dialogues. Put all of their knowledge together and you get a much more intricate understanding of the entire movie.</p>\n<h2>Why This Was a Big Deal</h2>\n<p>The results spoke for themselves:</p>\n<ul><li>On English-to-German translation, Transformers set a new state-of-the-art with a <strong>BLEU score of 28.4</strong></li><li>The model generalised well beyond translation, it was able to perform well in tasks like English parsing too</li></ul>\n<p>By solving sequence problems with attention, Transformers opened the door to models like <strong>BERT</strong>, <strong>GPT</strong>, and all the large language models we use today. That&#39;s why this one paper is cited everywhere... it marked the beginning of the modern AI wave. So, next time you hear <em>&quot;attention is all you need,&quot;</em> know it&#39;s not just a catchy phrase. It&#39;s also the research paper that revolutionised the AI landscape.</p>\n<p>And that&#39;s why the Transformers architecture <em>transformed</em> the tech world as we know of today (pun intended)</p>"
  },
  {
    "id": "khao-yai-thailand",
    "slug": "khao-yai-thailand",
    "title": "Khao Yai, Thailand",
    "excerpt": "My adventures in Khao Yai, and more importantly, the people who gave it meaning",
    "date": "2024-08-12",
    "category": "travels",
    "readTimeMinutes": 5,
    "content": [
      ">\"I-cha!, you know how to play poker mai?\"",
      "My head swivelled to the right.",
      "*I-cha.*",
      "P'X, my local friend, always had his own unique way of saying my name, owing to the Thai accent.",
      ">\"Err a little bit. You have the cards?\"",
      "I found myself standing outside the front door of his family home, an area bearing semblance of an outdoor porch. The space was vast, covered by a low roof to shade from Thailand's unforgiving sun. Tables and chairs scattered all around the porch, his family members moving about. We were at his vacation home. His family from all over gathered here in Khao Yai for a reunion, and he kindly invited me along with some other Singaporean friends to their home in Khao Yai. I found myself across the porch to where P'X was standing at a makeshift table. Glancing down, the table was littered with cards and cups, one for each of the aunties seated around the table. They were all looking up at me now. A mixture of curiosity and anticipation on their faces.",
      ">\"Sawadee ka, my name is Ayesha\"",
      "I smiled and greeted them with a slight bow. Respect was a big thing in Thailand, and I always made a point to show respect, especially towards elders.",
      ">\"I-cha! X friend! Welcome. Come sit na!\"",
      "They smiled back and tapped on an empty chair, beckoning me to sit. I found myself playing poker with them well into the night. Despite the langauge barrier, the friendly gamble served well enough as common ground. Laughing and talking over the game, they seemed more than content in the heat of the moment, playing poker on the lopsided table. A rickety set-up that contributed to an even better atmosphere. I looked up to see some people gathered around what seemed to be a grill. They were fanning it the old-school way whilst another set up an electric fan. Similarly, they were also laughing and drinking. It was a simple, refreshing atmosphere to say the least.",
      "Further down towards the house gates lay an open area with no shelter, where we later on sit around in a circle, on our camping chairs. We sat there talking about trivial things I don't even rememeber now. But what I do remember, is looking up at the sky to see it littered with stars. I've never seen so many of them packed together so closely before. I remember my gaze staying plastered on the sky. Not long after, a light breeze danced through the air as I closed my eyes to relish the moment. My local friends and their family opened their home to me. They treated me as their own. Made sure I was comfortable. I realised that my heart couldn't be fuller, sitting on an old chair that could barely hold my weight, eating skewers, drinking from $1 cans with foreign music blasting in the background. At that point, I too was content in the simplicity of it all.",
      "Khao Yai was one of my most memorable places in Thailand not beacuse it boasted the most beautiful sceneries or had the most extravagant culture. I have laid eyes on more novel attractions, more remarkable waterfalls, grander mountains. Yet, Khao Yai retains a soft spot in my heart, all beacuse of the people I experienced it with. My local friends proudly showed me around. Drove us in questionable vehicles (with my friend managing to fall off whilst we were moving. oops.) Little Thai tidbits of culture and language every now and then. Even their family would jump in to try and teach us too. All with a warm smile on their face.",
      "*I-cha*. A completely butchered pronounciation of my name, but it didn't bother me at all. In fact, I marvelled at the fact that I was even lucky enugh to be in such a position. A foreign country, foreign friends who have welcomed me into their family. A unqiue pronounciation — which the rest of his family adopted— that is now a token of kindness. Of hospitality. Of friendship that transcends beyond langauge barriers.",
      "Khop khun kha, P'X and P'Ung.",
      "![Project image](/writings/KY_1.jpg)"
    ],
    "markdown": ">\"I-cha!, you know how to play poker mai?\" \n\nMy head swivelled to the right. \n\n*I-cha.* \n\nP'X, my local friend, always had his own unique way of saying my name, owing to the Thai accent. \n\n>\"Err a little bit. You have the cards?\" \n\nI found myself standing outside the front door of his family home, an area bearing semblance of an outdoor porch. The space was vast, covered by a low roof to shade from Thailand's unforgiving sun. Tables and chairs scattered all around the porch, his family members moving about. We were at his vacation home. His family from all over gathered here in Khao Yai for a reunion, and he kindly invited me along with some other Singaporean friends to their home in Khao Yai. I found myself across the porch to where P'X was standing at a makeshift table. Glancing down, the table was littered with cards and cups, one for each of the aunties seated around the table. They were all looking up at me now. A mixture of curiosity and anticipation on their faces. \n\n>\"Sawadee ka, my name is Ayesha\" \n\nI smiled and greeted them with a slight bow. Respect was a big thing in Thailand, and I always made a point to show respect, especially towards elders.   \n\n>\"I-cha! X friend! Welcome. Come sit na!\" \n\nThey smiled back and tapped on an empty chair, beckoning me to sit. I found myself playing poker with them well into the night. Despite the langauge barrier, the friendly gamble served well enough as common ground. Laughing and talking over the game, they seemed more than content in the heat of the moment, playing poker on the lopsided table. A rickety set-up that contributed to an even better atmosphere. I looked up to see some people gathered around what seemed to be a grill. They were fanning it the old-school way whilst another set up an electric fan. Similarly, they were also laughing and drinking. It was a simple, refreshing atmosphere to say the least. \n\nFurther down towards the house gates lay an open area with no shelter, where we later on sit around in a circle, on our camping chairs. We sat there talking about trivial things I don't even rememeber now. But what I do remember, is looking up at the sky to see it littered with stars. I've never seen so many of them packed together so closely before. I remember my gaze staying plastered on the sky. Not long after, a light breeze danced through the air as I closed my eyes to relish the moment. My local friends and their family opened their home to me. They treated me as their own. Made sure I was comfortable. I realised that my heart couldn't be fuller, sitting on an old chair that could barely hold my weight, eating skewers, drinking from $1 cans with foreign music blasting in the background. At that point, I too was content in the simplicity of it all.\n\nKhao Yai was one of my most memorable places in Thailand not beacuse it boasted the most beautiful sceneries or had the most extravagant culture. I have laid eyes on more novel attractions, more remarkable waterfalls, grander mountains. Yet, Khao Yai retains a soft spot in my heart, all beacuse of the people I experienced it with. My local friends proudly showed me around. Drove us in questionable vehicles (with my friend managing to fall off whilst we were moving. oops.) Little Thai tidbits of culture and language every now and then. Even their family would jump in to try and teach us too. All with a warm smile on their face. \n\n*I-cha*. A completely butchered pronounciation of my name, but it didn't bother me at all. In fact, I marvelled at the fact that I was even lucky enugh to be in such a position. A foreign country, foreign friends who have welcomed me into their family. A unqiue pronounciation — which the rest of his family adopted— that is now a token of kindness. Of hospitality. Of friendship that transcends beyond langauge barriers.\n\nKhop khun kha, P'X and P'Ung.\n\n![Project image](/writings/KY_1.jpg)",
    "html": "<blockquote><p>&quot;I-cha!, you know how to play poker mai?&quot; </p></blockquote>\n<p>My head swivelled to the right.</p>\n<p><em>I-cha.</em></p>\n<p>P&#39;X, my local friend, always had his own unique way of saying my name, owing to the Thai accent.</p>\n<blockquote><p>&quot;Err a little bit. You have the cards?&quot; </p></blockquote>\n<p>I found myself standing outside the front door of his family home, an area bearing semblance of an outdoor porch. The space was vast, covered by a low roof to shade from Thailand&#39;s unforgiving sun. Tables and chairs scattered all around the porch, his family members moving about. We were at his vacation home. His family from all over gathered here in Khao Yai for a reunion, and he kindly invited me along with some other Singaporean friends to their home in Khao Yai. I found myself across the porch to where P&#39;X was standing at a makeshift table. Glancing down, the table was littered with cards and cups, one for each of the aunties seated around the table. They were all looking up at me now. A mixture of curiosity and anticipation on their faces.</p>\n<blockquote><p>&quot;Sawadee ka, my name is Ayesha&quot; </p></blockquote>\n<p>I smiled and greeted them with a slight bow. Respect was a big thing in Thailand, and I always made a point to show respect, especially towards elders.</p>\n<blockquote><p>&quot;I-cha! X friend! Welcome. Come sit na!&quot; </p></blockquote>\n<p>They smiled back and tapped on an empty chair, beckoning me to sit. I found myself playing poker with them well into the night. Despite the langauge barrier, the friendly gamble served well enough as common ground. Laughing and talking over the game, they seemed more than content in the heat of the moment, playing poker on the lopsided table. A rickety set-up that contributed to an even better atmosphere. I looked up to see some people gathered around what seemed to be a grill. They were fanning it the old-school way whilst another set up an electric fan. Similarly, they were also laughing and drinking. It was a simple, refreshing atmosphere to say the least.</p>\n<p>Further down towards the house gates lay an open area with no shelter, where we later on sit around in a circle, on our camping chairs. We sat there talking about trivial things I don&#39;t even rememeber now. But what I do remember, is looking up at the sky to see it littered with stars. I&#39;ve never seen so many of them packed together so closely before. I remember my gaze staying plastered on the sky. Not long after, a light breeze danced through the air as I closed my eyes to relish the moment. My local friends and their family opened their home to me. They treated me as their own. Made sure I was comfortable. I realised that my heart couldn&#39;t be fuller, sitting on an old chair that could barely hold my weight, eating skewers, drinking from $1 cans with foreign music blasting in the background. At that point, I too was content in the simplicity of it all.</p>\n<p>Khao Yai was one of my most memorable places in Thailand not beacuse it boasted the most beautiful sceneries or had the most extravagant culture. I have laid eyes on more novel attractions, more remarkable waterfalls, grander mountains. Yet, Khao Yai retains a soft spot in my heart, all beacuse of the people I experienced it with. My local friends proudly showed me around. Drove us in questionable vehicles (with my friend managing to fall off whilst we were moving. oops.) Little Thai tidbits of culture and language every now and then. Even their family would jump in to try and teach us too. All with a warm smile on their face.</p>\n<p><em>I-cha</em>. A completely butchered pronounciation of my name, but it didn&#39;t bother me at all. In fact, I marvelled at the fact that I was even lucky enugh to be in such a position. A foreign country, foreign friends who have welcomed me into their family. A unqiue pronounciation — which the rest of his family adopted— that is now a token of kindness. Of hospitality. Of friendship that transcends beyond langauge barriers.</p>\n<p>Khop khun kha, P&#39;X and P&#39;Ung.</p>\n<p><img src=\"/writings/KY_1.jpg\" alt=\"Project image\" /></p>"
  },
  {
    "id": "addie-larue",
    "slug": "addie-larue",
    "title": "The Invisible Life of Addie Larue",
    "excerpt": "My notes on one of my favourite books about perspective",
    "date": "2023-08-10",
    "category": "literature",
    "readTimeMinutes": 10,
    "content": [
      "## A short summary of the book \nThe story begins in 1714, France, when a woman named Addie Larue is tricked into making a deal with a dark god, Luc. He grants her immortality, but cursed to be **forgotten by everyone she meets**, thereby making her live an *invisible* life for years to come.",
      "For centuries Addie lives in solitude, with Luc visiting her on their 'anniversaries' to torment her into giving up her soul to him. However, Addie remains steadfast and grows to resent Luc for his constant manipulation. Eventually, Luc becomes the only constant in Addie's world of temporary connections. She finds herself looking forward to his visits, wrestling with feelings that toe between hatred and growing affection. Addie and Luc engage in a short romantic stint, which ends horribly. Luc does not visit her for years to come.",
      "Addie then meets Henry — the ++first person in 300 years who remembers her++. They fall in love, only for Addie to discover the truth: Henry too made a deal with Luc, and his time is running out.",
      "At first, Addie thinks Luc messed up by allowing the two of them to meet, only to find out that they played right into his plans. He wanted Addie to fall in love with Henry — and then be forced to grieve him when he dies in a few weeks. Luc orchestrated this entire plot to show Addie that mortal love is fragile and not worth the pain. He wanted Addie to run back to him.",
      "However, Luc's plan backfires. Addie remains stubborn and is instead all the more determined to make full use of her remaining time with Henry.",
      "On Henry's final night, Addie reveals that she made another deal with Luc to save Henry. The catch is that Addie must go with Luc. Henry is devastated, but Addie tells him she has lived long enough, and it is now Henry's turn to make the most of his. She makes Henry promise to remember her, before vanishing.",
      "> \"'You better live a good life, Henry Strauss' She begins to pull away, but his grip tightens. 'No'. She sighs, 'You've given me so much, Henry. But I need you to do one more thing.' Her forehead presses against his. '**I need you to remember.**'\"",
      "The story ends off with Addie walking by a bookstore, parallelism of where Henry and her first met. She observes a book: \"The invisible life of Addie Larue\" written by Henry. Though she was with Luc, inwardly she finds comfort in that she has tricked Luc the same way he tricked her all those years before: she did not tell him that she would be his forever — only up till he **no longer wanted her**, and she will do everything she can to make him stop wanting her. For now, Addie says nothing and only smiles as Luc pulls her into his arms.",
      "That is how the book ends, leaving the ending up for interpretation.",
      "## My thoughts",
      "From this, we can gather that in the end, Addie goes with Luc, and Henry writes a book about her titled 'The Invisible Life of Addie Larue'.",
      "I think it's a brilliant book. The ending may not sit well for others but in my opinion, it is clever and open-ended. I think many readers take for granted the revelation that Henry is the author of the book, and the world of possibilities it opens up.",
      "Because this makes it plausible that Addie's story is not told in total objectivity.",
      "## Henry's ending vs the Truth\nThe final chapter tells Addie's careful wording of her deal with Luc: \n> 'I'll be yours as long as you want me by your side'",
      "along with her supposed plan to make Luc hate her over time. In my opinion, this is merely Henry's interpretation of what had happened. This is his ending. The ending he wishes for.",
      "The realisation that the book is from Henry's perspective makes the reader ponder whether Addie's accounts were truly objective or if they were coloured with Henry's desires — with what he *wanted* to happen — he *wanted* Addie to be reluctant to go with Luc, that she only went with him in the name of saving Henry. He *wanted* to remember her as noble, as someone who reciprocated his love.",
      "*The truth is, Henry does not know at all*",
      "## Addie and Luc: More than Enemies?\nConsidering that Addie's story is told through Henry's lens, we can gather that Addie told Henry about her erratic relationship with Luc. Their relationship is explored in small fragments, from sensations of betrayal to fascination.",
      "This makes one wonder whether the short-lived bouts of romantic interest for Luc sprinkled few and far between throughout the book is truly insignificant — or if it is instead just a projection of what Henry *wants* it to be.",
      "Luc is humanised more and more as the story unfolds. He became capable of love for Addie, with his regular visits, and even admits to enjoying her company. Addie, in turn, also wonders about her feelings for Luc.",
      "The book shows Addie trying to convince herself it is not love. Then again, I think this is justified. Given their tumultous connection over 300 years, the line between love and obsession can be easily blurred. However, one can also interpret it as Addie denying it so fiercely because deep down she knows she caught intense feelings for him, feelings that could even be much stronger than those for Henry.",
      "## A Happy Ending in Disguise?\nIn the book Addie admits that she is no longer human. By extension, does that mean she is no longer capable of *human* love? Well, after centuries of immortality, her perspective has definitely shifted. Maybe Luc is the only one truly capable of understanding her.",
      "If that is the case, maybe her ending with Luc isn't tragic at all. She gets freedom from loneliness, a partnership with someone who matches her in wit, cunning and immortality. But since the book is told in Henry's point of view, it makes sense to end it off with Addie's vengeance and hatred for Luc.",
      "For Henry, it is tragic. For Addie, it could be liberation, with a hint of danger.",
      "## Why I Love the Ending\nThe ending of the book is thought-provoking and that's what I love about it. Schwab leaves us to piece together our own version of events from the book's tiny cues and interpretations of the characters' motives.",
      "You could take the ending purely as it is from Henry's perspective, reading a story of loss, darkness and self-sacrifice. On the other hand, you can also see it as a story of freedom, manipulation and unexpected companionship.",
      "The ending is an incredible interplay of perspective and loss. But I personally think that the main point of Addie's story was never about who won and who lost, but about how we *choose* to remember her — and the 'truths' we tell ourselves to make the ending feel complete."
    ],
    "markdown": "## A short summary of the book \nThe story begins in 1714, France, when a woman named Addie Larue is tricked into making a deal with a dark god, Luc. He grants her immortality, but cursed to be **forgotten by everyone she meets**, thereby making her live an *invisible* life for years to come. \n\nFor centuries Addie lives in solitude, with Luc visiting her on their 'anniversaries' to torment her into giving up her soul to him. However, Addie remains steadfast and grows to resent Luc for his constant manipulation. Eventually, Luc becomes the only constant in Addie's world of temporary connections. She finds herself looking forward to his visits, wrestling with feelings that toe between hatred and growing affection. Addie and Luc engage in a short romantic stint, which ends horribly. Luc does not visit her for years to come. \n\nAddie then meets Henry — the ++first person in 300 years who remembers her++. They fall in love, only for Addie to discover the truth: Henry too made a deal with Luc, and his time is running out.\n\nAt first, Addie thinks Luc messed up by allowing the two of them to meet, only to find out that they played right into his plans. He wanted Addie to fall in love with Henry — and then be forced to grieve him when he dies in a few weeks. Luc orchestrated this entire plot to show Addie that mortal love is fragile and not worth the pain. He wanted Addie to run back to him. \n\nHowever, Luc's plan backfires. Addie remains stubborn and is instead all the more determined to make full use of her remaining time with Henry. \n\nOn Henry's final night, Addie reveals that she made another deal with Luc to save Henry. The catch is that Addie must go with Luc. Henry is devastated, but Addie tells him she has lived long enough, and it is now Henry's turn to make the most of his. She makes Henry promise to remember her, before vanishing.\n\n> \"'You better live a good life, Henry Strauss' She begins to pull away, but his grip tightens. 'No'. She sighs, 'You've given me so much, Henry. But I need you to do one more thing.' Her forehead presses against his. '**I need you to remember.**'\"\n\nThe story ends off with Addie walking by a bookstore, parallelism of where Henry and her first met. She observes a book: \"The invisible life of Addie Larue\" written by Henry. Though she was with Luc, inwardly she finds comfort in that she has tricked Luc the same way he tricked her all those years before: she did not tell him that she would be his forever — only up till he **no longer wanted her**, and she will do everything she can to make him stop wanting her. For now, Addie says nothing and only smiles as Luc pulls her into his arms. \n\nThat is how the book ends, leaving the ending up for interpretation.\n\n## My thoughts\n\nFrom this, we can gather that in the end, Addie goes with Luc, and Henry writes a book about her titled 'The Invisible Life of Addie Larue'. \n\nI think it's a brilliant book. The ending may not sit well for others but in my opinion, it is clever and open-ended. I think many readers take for granted the revelation that Henry is the author of the book, and the world of possibilities it opens up.\n\nBecause this makes it plausible that Addie's story is not told in total objectivity.\n\n## Henry's ending vs the Truth\nThe final chapter tells Addie's careful wording of her deal with Luc: \n> 'I'll be yours as long as you want me by your side'\n\nalong with her supposed plan to make Luc hate her over time. In my opinion, this is merely Henry's interpretation of what had happened. This is his ending. The ending he wishes for.\n\nThe realisation that the book is from Henry's perspective makes the reader ponder whether Addie's accounts were truly objective or if they were coloured with Henry's desires — with what he *wanted* to happen — he *wanted* Addie to be reluctant to go with Luc, that she only went with him in the name of saving Henry. He *wanted* to remember her as noble, as someone who reciprocated his love.\n\n*The truth is, Henry does not know at all* \n\n## Addie and Luc: More than Enemies?\nConsidering that Addie's story is told through Henry's lens, we can gather that Addie told Henry about her erratic relationship with Luc. Their relationship is explored in small fragments, from sensations of betrayal to fascination.\n\nThis makes one wonder whether the short-lived bouts of romantic interest for Luc sprinkled few and far between throughout the book is truly insignificant — or if it is instead just a projection of what Henry *wants* it to be. \n\nLuc is humanised more and more as the story unfolds. He became capable of love for Addie, with his regular visits, and even admits to enjoying her company. Addie, in turn, also wonders about her feelings for Luc. \n\nThe book shows Addie trying to convince herself it is not love. Then again, I think this is justified. Given their tumultous connection over 300 years, the line between love and obsession can be easily blurred. However, one can also interpret it as Addie denying it so fiercely because deep down she knows she caught intense feelings for him, feelings that could even be much stronger than those for Henry. \n\n## A Happy Ending in Disguise?\nIn the book Addie admits that she is no longer human. By extension, does that mean she is no longer capable of *human* love? Well, after centuries of immortality, her perspective has definitely shifted. Maybe Luc is the only one truly capable of understanding her. \n\nIf that is the case, maybe her ending with Luc isn't tragic at all. She gets freedom from loneliness, a partnership with someone who matches her in wit, cunning and immortality. But since the book is told in Henry's point of view, it makes sense to end it off with Addie's vengeance and hatred for Luc. \n\nFor Henry, it is tragic. For Addie, it could be liberation, with a hint of danger.\n\n## Why I Love the Ending\nThe ending of the book is thought-provoking and that's what I love about it. Schwab leaves us to piece together our own version of events from the book's tiny cues and interpretations of the characters' motives.\n\nYou could take the ending purely as it is from Henry's perspective, reading a story of loss, darkness and self-sacrifice. On the other hand, you can also see it as a story of freedom, manipulation and unexpected companionship. \n\nThe ending is an incredible interplay of perspective and loss. But I personally think that the main point of Addie's story was never about who won and who lost, but about how we *choose* to remember her — and the 'truths' we tell ourselves to make the ending feel complete.",
    "html": "<h2>A short summary of the book </h2>\n<p>The story begins in 1714, France, when a woman named Addie Larue is tricked into making a deal with a dark god, Luc. He grants her immortality, but cursed to be <strong>forgotten by everyone she meets</strong>, thereby making her live an <em>invisible</em> life for years to come.</p>\n<p>For centuries Addie lives in solitude, with Luc visiting her on their &#39;anniversaries&#39; to torment her into giving up her soul to him. However, Addie remains steadfast and grows to resent Luc for his constant manipulation. Eventually, Luc becomes the only constant in Addie&#39;s world of temporary connections. She finds herself looking forward to his visits, wrestling with feelings that toe between hatred and growing affection. Addie and Luc engage in a short romantic stint, which ends horribly. Luc does not visit her for years to come.</p>\n<p>Addie then meets Henry — the <u>first person in 300 years who remembers her</u>. They fall in love, only for Addie to discover the truth: Henry too made a deal with Luc, and his time is running out.</p>\n<p>At first, Addie thinks Luc messed up by allowing the two of them to meet, only to find out that they played right into his plans. He wanted Addie to fall in love with Henry — and then be forced to grieve him when he dies in a few weeks. Luc orchestrated this entire plot to show Addie that mortal love is fragile and not worth the pain. He wanted Addie to run back to him.</p>\n<p>However, Luc&#39;s plan backfires. Addie remains stubborn and is instead all the more determined to make full use of her remaining time with Henry.</p>\n<p>On Henry&#39;s final night, Addie reveals that she made another deal with Luc to save Henry. The catch is that Addie must go with Luc. Henry is devastated, but Addie tells him she has lived long enough, and it is now Henry&#39;s turn to make the most of his. She makes Henry promise to remember her, before vanishing.</p>\n<blockquote><p>&quot;&#39;You better live a good life, Henry Strauss&#39; She begins to pull away, but his grip tightens. &#39;No&#39;. She sighs, &#39;You&#39;ve given me so much, Henry. But I need you to do one more thing.&#39; Her forehead presses against his. &#39;<strong>I need you to remember.</strong>&#39;&quot;</p></blockquote>\n<p>The story ends off with Addie walking by a bookstore, parallelism of where Henry and her first met. She observes a book: &quot;The invisible life of Addie Larue&quot; written by Henry. Though she was with Luc, inwardly she finds comfort in that she has tricked Luc the same way he tricked her all those years before: she did not tell him that she would be his forever — only up till he <strong>no longer wanted her</strong>, and she will do everything she can to make him stop wanting her. For now, Addie says nothing and only smiles as Luc pulls her into his arms.</p>\n<p>That is how the book ends, leaving the ending up for interpretation.</p>\n<h2>My thoughts</h2>\n<p>From this, we can gather that in the end, Addie goes with Luc, and Henry writes a book about her titled &#39;The Invisible Life of Addie Larue&#39;.</p>\n<p>I think it&#39;s a brilliant book. The ending may not sit well for others but in my opinion, it is clever and open-ended. I think many readers take for granted the revelation that Henry is the author of the book, and the world of possibilities it opens up.</p>\n<p>Because this makes it plausible that Addie&#39;s story is not told in total objectivity.</p>\n<h2>Henry&#39;s ending vs the Truth</h2>\n<p>The final chapter tells Addie&#39;s careful wording of her deal with Luc:</p>\n<blockquote><p>&#39;I&#39;ll be yours as long as you want me by your side&#39;</p></blockquote>\n<p>along with her supposed plan to make Luc hate her over time. In my opinion, this is merely Henry&#39;s interpretation of what had happened. This is his ending. The ending he wishes for.</p>\n<p>The realisation that the book is from Henry&#39;s perspective makes the reader ponder whether Addie&#39;s accounts were truly objective or if they were coloured with Henry&#39;s desires — with what he <em>wanted</em> to happen — he <em>wanted</em> Addie to be reluctant to go with Luc, that she only went with him in the name of saving Henry. He <em>wanted</em> to remember her as noble, as someone who reciprocated his love.</p>\n<p><em>The truth is, Henry does not know at all</em></p>\n<h2>Addie and Luc: More than Enemies?</h2>\n<p>Considering that Addie&#39;s story is told through Henry&#39;s lens, we can gather that Addie told Henry about her erratic relationship with Luc. Their relationship is explored in small fragments, from sensations of betrayal to fascination.</p>\n<p>This makes one wonder whether the short-lived bouts of romantic interest for Luc sprinkled few and far between throughout the book is truly insignificant — or if it is instead just a projection of what Henry <em>wants</em> it to be.</p>\n<p>Luc is humanised more and more as the story unfolds. He became capable of love for Addie, with his regular visits, and even admits to enjoying her company. Addie, in turn, also wonders about her feelings for Luc.</p>\n<p>The book shows Addie trying to convince herself it is not love. Then again, I think this is justified. Given their tumultous connection over 300 years, the line between love and obsession can be easily blurred. However, one can also interpret it as Addie denying it so fiercely because deep down she knows she caught intense feelings for him, feelings that could even be much stronger than those for Henry.</p>\n<h2>A Happy Ending in Disguise?</h2>\n<p>In the book Addie admits that she is no longer human. By extension, does that mean she is no longer capable of <em>human</em> love? Well, after centuries of immortality, her perspective has definitely shifted. Maybe Luc is the only one truly capable of understanding her.</p>\n<p>If that is the case, maybe her ending with Luc isn&#39;t tragic at all. She gets freedom from loneliness, a partnership with someone who matches her in wit, cunning and immortality. But since the book is told in Henry&#39;s point of view, it makes sense to end it off with Addie&#39;s vengeance and hatred for Luc.</p>\n<p>For Henry, it is tragic. For Addie, it could be liberation, with a hint of danger.</p>\n<h2>Why I Love the Ending</h2>\n<p>The ending of the book is thought-provoking and that&#39;s what I love about it. Schwab leaves us to piece together our own version of events from the book&#39;s tiny cues and interpretations of the characters&#39; motives.</p>\n<p>You could take the ending purely as it is from Henry&#39;s perspective, reading a story of loss, darkness and self-sacrifice. On the other hand, you can also see it as a story of freedom, manipulation and unexpected companionship.</p>\n<p>The ending is an incredible interplay of perspective and loss. But I personally think that the main point of Addie&#39;s story was never about who won and who lost, but about how we <em>choose</em> to remember her — and the &#39;truths&#39; we tell ourselves to make the ending feel complete.</p>"
  }
];

export function getWritingBySlug(slug) {
  return writings.find((w) => w.slug === slug);
}
